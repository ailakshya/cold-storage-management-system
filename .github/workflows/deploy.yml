name: Deploy Production

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: [self-hosted, cold-server-runner]
    timeout-minutes: 15
    env:
      APP_DIR: /home/cold/cold-storage
      IMAGE_NAME: cold-backend

    steps:
      - name: Clean Workspace
        run: |
          rm -rf ./* ./.git ./.github || true

      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Prepare Assets
        run: |
          mkdir -p static
          cp templates/portfolio.html static/index.html

      - name: Set Image Tag
        id: tag
        run: |
          SHA=$(git rev-parse --short HEAD)
          echo "sha=$SHA" >> "$GITHUB_OUTPUT"
          echo "Image tag: $SHA"

      - name: Storage Pre-Flight Check
        run: |
          echo "=== Storage Health (ZFS + mdraid) ==="
          bash scripts/zfs-health-check.sh --deploy-gate || {
            EXIT=$?
            if [ $EXIT -ge 2 ]; then
              echo "::error::Storage health critical — aborting deploy"
              exit 1
            fi
            # exit 1 = degraded, allow deploy with warning
          }

          echo "=== Verifying Mount Points ==="
          # App volumes — must be owned by UID 1000 (appuser)
          for DIR in /mass-pool/shared /mass-pool/archives /mass-pool/trash /mass-pool/backups /fast-pool/data; do
            if [ ! -d "$DIR" ]; then
              echo "::error::Mount point $DIR does not exist"
              exit 1
            fi
            OWNER=$(stat -c '%u' "$DIR")
            if [ "$OWNER" != "1000" ]; then
              echo "::warning::$DIR owned by UID $OWNER, fixing to 1000..."
              sudo chown 1000:1000 "$DIR"
            fi
          done

          # TimescaleDB volume — must be owned by UID 70 (postgres inside container)
          if [ -d "/fast-pool/timescaledb" ]; then
            OWNER=$(stat -c '%u' "/fast-pool/timescaledb")
            if [ "$OWNER" != "70" ]; then
              echo "::warning::/fast-pool/timescaledb owned by UID $OWNER, fixing to 70 (postgres)..."
              sudo chown -R 70:70 /fast-pool/timescaledb
            fi
          fi

          # Write test on each pool
          touch /mass-pool/shared/.deploy-test && rm /mass-pool/shared/.deploy-test
          touch /fast-pool/data/.deploy-test && rm /fast-pool/data/.deploy-test
          echo "Mount points verified: all paths exist, writable, correct ownership"

      - name: Build Docker Image
        run: |
          DOCKER_BUILDKIT=1 docker build \
            -f Dockerfile.production \
            -t "$IMAGE_NAME:${{ steps.tag.outputs.sha }}" \
            -t "$IMAGE_NAME:latest" \
            .

      - name: Verify Environment File
        run: |
          if [ ! -f "$APP_DIR/.env.production" ]; then
            echo "::error::.env.production not found at $APP_DIR/.env.production"
            echo "Create it with: DB_USER, DB_PASSWORD, DB_NAME, JWT_SECRET, METRICS_DB_USER, METRICS_DB_PASSWORD, METRICS_DB_NAME"
            exit 1
          fi

      - name: Configure pg_hba for Docker Bridge
        run: |
          # Allow Docker containers to connect to host PostgreSQL (one-time, idempotent)
          PG_HBA=$(sudo -u postgres psql -t -P format=unaligned -c 'SHOW hba_file' 2>/dev/null || echo "")
          if [ -n "$PG_HBA" ] && [ -f "$PG_HBA" ]; then
            if ! sudo grep -q "172.16.0.0/12" "$PG_HBA"; then
              echo "Adding Docker bridge subnet to pg_hba.conf..."
              sudo sed -i '1s|^|host    all             all             172.16.0.0/12           md5\n|' "$PG_HBA"
              sudo systemctl reload postgresql
              echo "pg_hba.conf updated and PostgreSQL reloaded"
            else
              echo "Docker bridge subnet already in pg_hba.conf"
            fi
          else
            echo "::warning::Could not locate pg_hba.conf — verify Docker-to-PostgreSQL connectivity manually"
          fi

      - name: Stop Legacy Services (free ports for containers)
        run: |
          # Stop bare-metal systemd services that conflict with Docker container ports
          for SVC in cold-employee cold-customer cold-website cold-monitoring redis redis-server; do
            if systemctl is-active --quiet "$SVC" 2>/dev/null; then
              echo "Stopping legacy service: $SVC"
              sudo systemctl stop "$SVC"
              sudo systemctl disable "$SVC" 2>/dev/null || true
            fi
          done

          # Stop and remove any old Docker containers from previous compose setups
          for OLD_CT in cold-customer cold-website cold-employee cold-storage-management-system-app-base-1; do
            if docker ps -a --format '{{.Names}}' | grep -q "^${OLD_CT}$"; then
              echo "Removing old container: $OLD_CT"
              docker stop "$OLD_CT" 2>/dev/null || true
              docker rm "$OLD_CT" 2>/dev/null || true
            fi
          done

          # Remove failed containers from previous deploy attempts
          for CT in cold-backend cold-redis cold-timescaledb; do
            STATUS=$(docker inspect --format='{{.State.Status}}' "$CT" 2>/dev/null || echo "")
            if [ "$STATUS" = "created" ] || [ "$STATUS" = "exited" ]; then
              echo "Removing stale container: $CT ($STATUS)"
              docker rm "$CT" 2>/dev/null || true
            fi
          done

          # Kill any remaining process holding critical ports
          for PORT in 6379 8080 8081 8082 9090; do
            if ss -tlnp | grep -q ":${PORT} "; then
              echo "Port $PORT still in use — killing process..."
              sudo fuser -k ${PORT}/tcp 2>/dev/null || true
            fi
          done
          sleep 2

          echo "Legacy services stopped, ports freed"

      - name: Deploy with Docker Compose
        run: |
          # Copy production files to app directory
          cp docker-compose.production.yml "$APP_DIR/"
          cp Dockerfile.production "$APP_DIR/"
          cp Dockerfile.caddy "$APP_DIR/" 2>/dev/null || true
          cp -r scripts "$APP_DIR/" 2>/dev/null || true
          cp -r configs "$APP_DIR/" 2>/dev/null || true

          # Deploy — only recreates containers whose image/config changed
          # Redis and TimescaleDB stay running if unchanged (zero downtime for them)
          cd "$APP_DIR"
          IMAGE_TAG="${{ steps.tag.outputs.sha }}" \
            docker compose -f docker-compose.production.yml \
            --env-file .env.production \
            up -d

          echo "Containers launched — waiting for health checks..."

      - name: Wait for Health
        run: |
          echo "Polling container health..."
          MAX_WAIT=90
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            HEALTH=$(docker inspect --format='{{.State.Health.Status}}' cold-backend 2>/dev/null || echo "starting")
            if [ "$HEALTH" = "healthy" ]; then
              echo "Container healthy after ${ELAPSED}s"
              break
            fi
            echo "  Status: $HEALTH (${ELAPSED}s/${MAX_WAIT}s)"
            sleep 5
            ELAPSED=$((ELAPSED + 5))
          done

          if [ "$HEALTH" != "healthy" ]; then
            echo "::error::Container did not become healthy within ${MAX_WAIT}s"
            echo "=== Container Logs ==="
            docker logs cold-backend --tail 50
            exit 1
          fi

          # Verify all ports respond
          echo "=== Port Verification ==="
          # Employee (8080) and Customer (8081) have /health endpoints
          for PORT in 8080 8081; do
            if curl -sf --max-time 5 "http://localhost:${PORT}/health" > /dev/null 2>&1; then
              echo "  Port $PORT: OK"
            else
              echo "::error::Port $PORT not responding"
              docker logs cold-backend --tail 30
              exit 1
            fi
          done
          # Website (8082) serves HTML, check with HTTP status instead of /health
          if curl -so /dev/null -w '%{http_code}' --max-time 5 "http://localhost:8082/" | grep -qE '^(200|301|302)$'; then
            echo "  Port 8082: OK"
          else
            echo "::error::Port 8082 not responding"
            docker logs cold-backend --tail 30
            exit 1
          fi
          echo "All ports healthy"

      - name: Verify Cloudflare Tunnel
        run: |
          # Tunnel uses token-based config managed via Cloudflare dashboard
          # Just verify the service is running
          if systemctl is-active --quiet cloudflared 2>/dev/null; then
            echo "Cloudflare tunnel: active"
          else
            echo "::warning::Cloudflare tunnel service is not running"
            echo "Start it with: sudo systemctl start cloudflared"
          fi

      - name: Cleanup Old Images
        run: |
          # Keep last 3 tagged images, prune the rest
          docker image ls "$IMAGE_NAME" --format '{{.Tag}}' | \
            grep -vE '^latest$' | sort -r | tail -n +4 | \
            xargs -I{} docker image rm "$IMAGE_NAME:{}" 2>/dev/null || true
          docker image prune -f > /dev/null 2>&1 || true

      - name: Deployment Summary
        if: always()
        run: |
          echo "============================================"
          echo "  Deployment Complete"
          echo "============================================"
          echo "  Image:   $IMAGE_NAME:${{ steps.tag.outputs.sha }}"
          echo "  Branch:  ${{ github.ref_name }}"
          echo "  Commit:  ${{ github.sha }}"
          echo ""
          echo "=== Container Status ==="
          docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "cold-|NAME" || true
          echo ""
          echo "=== ZFS Pool Status ==="
          zpool list 2>/dev/null || echo "zpool command not available"
          echo ""
          echo "=== Endpoints ==="
          echo "  Employee:  https://app.gurukripacoldstore.in"
          echo "  Customer:  https://customer.gurukripacoldstore.in"
          echo "  Website:   https://gurukripacoldstore.in"
          echo "============================================"
